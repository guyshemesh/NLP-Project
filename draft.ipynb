{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ8qof0-2qBP"
      },
      "source": [
        "italicized text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AhTU71pp2snb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59393afd-be9d-46f5-ff08-e1d5fd2a30d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch matplotlib evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SApQmUXr2sLO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnCYQeaH1-KZ",
        "outputId": "dbd2212f-8ebf-4b96-eaf9-b4ff9ebfc346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "# Load the SST-2, SQuAD, and CoNLL-2003 datasets\n",
        "dataset_sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "dataset_squad = load_dataset(\"squad\")\n",
        "dataset_conll = load_dataset(\"conll2003\")\n",
        "\n",
        "# Load metric (accuracy)\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fMeN_cZp3Pc2"
      },
      "outputs": [],
      "source": [
        "# Tokenization for SST-2\n",
        "def tokenize_sst2(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_sst2 = dataset_sst2.map(tokenize_sst2, batched=True)\n",
        "train_sst2 = tokenized_sst2[\"train\"]\n",
        "eval_sst2 = tokenized_sst2[\"validation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L8YOCGmD5A2V"
      },
      "outputs": [],
      "source": [
        "# Tokenization for SQuAD\n",
        "def tokenize_squad(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"question\"], examples[\"context\"], truncation=True, padding=\"max_length\", max_length=384\n",
        "    )\n",
        "\n",
        "tokenized_squad = dataset_squad.map(tokenize_squad, batched=True)\n",
        "train_squad = tokenized_squad[\"train\"]\n",
        "eval_squad = tokenized_squad[\"validation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gfdh82vz64T3"
      },
      "outputs": [],
      "source": [
        "# Tokenization for CoNLL-2003\n",
        "def tokenize_conll(examples):\n",
        "    return tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "tokenized_conll = dataset_conll.map(tokenize_conll, batched=True)\n",
        "train_conll = tokenized_conll[\"train\"]\n",
        "eval_conll = tokenized_conll[\"validation\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ruwUuQkF64Os"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertForQuestionAnswering, Trainer, TrainingArguments\n",
        "\n",
        "# Define the modified model class to allow adding different types of layers\n",
        "class ModifiedBertModel(nn.Module):\n",
        "    def __init__(self, original_model, additional_attn_layers=0, additional_ff_layers=0, additional_embed_layers=0):\n",
        "        super(ModifiedBertModel, self).__init__()\n",
        "        self.bert = original_model.bert  # Use the pre-trained BERT model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = original_model.classifier\n",
        "\n",
        "        # Add additional self-attention layers\n",
        "        self.extra_attn_layers = nn.ModuleList(\n",
        "            [nn.TransformerEncoderLayer(d_model=768, nhead=12) for _ in range(additional_attn_layers)]\n",
        "        )\n",
        "\n",
        "        # Add additional feed-forward layers\n",
        "        self.extra_ff_layers = nn.ModuleList(\n",
        "            [nn.Linear(768, 768) for _ in range(additional_ff_layers)]\n",
        "        )\n",
        "\n",
        "        # Add additional embedding layers\n",
        "        self.extra_embed_layers = nn.ModuleList(\n",
        "            [nn.Embedding(30522, 768) for _ in range(additional_embed_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Pass through additional embedding layers\n",
        "        for layer in self.extra_embed_layers:\n",
        "            sequence_output = layer(input_ids)\n",
        "\n",
        "        # Pass through additional self-attention layers\n",
        "        for layer in self.extra_attn_layers:\n",
        "            sequence_output = layer(sequence_output)\n",
        "\n",
        "        # Pass through additional feed-forward layers\n",
        "        for layer in self.extra_ff_layers:\n",
        "            sequence_output = layer(sequence_output)\n",
        "\n",
        "        pooled_output = sequence_output[:, 0]  # Taking the [CLS] token's representation\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ebIBvthe64Fy"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import os\n",
        "\n",
        "# Function to train the model and use checkpointing\n",
        "def train_with_checkpointing(task_name, model, train_dataset, eval_dataset, output_dir, num_epochs=2, additional_layers=0):\n",
        "    # Check if there is an existing checkpoint\n",
        "    last_checkpoint = None\n",
        "    if os.path.exists(output_dir) and len(os.listdir(output_dir)) > 0:\n",
        "        last_checkpoint = output_dir\n",
        "        print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"steps\",  # Make eval strategy match save strategy\n",
        "        save_strategy=\"steps\",  # Set save strategy to steps\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=num_epochs,\n",
        "        save_steps=500,  # Save the model every 500 steps\n",
        "        save_total_limit=3,  # Keep only the last 3 checkpoints\n",
        "        logging_dir=f\"./logs_{task_name}\",\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=True,  # Load the best model after training\n",
        "        resume_from_checkpoint=last_checkpoint if last_checkpoint else None,  # Resume training from the last checkpoint\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=lambda p: metric.compute(predictions=p.predictions.argmax(-1), references=p.label_ids),\n",
        "    )\n",
        "\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result['eval_accuracy']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w-xWjfRp7sXY"
      },
      "outputs": [],
      "source": [
        "def train_sst2_with_layers(num_layers):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "    modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "\n",
        "    output_dir = f\"./sst2_checkpoints_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"sst2\", modified_model, train_sst2, eval_sst2, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dpLw4Pge7sRZ"
      },
      "outputs": [],
      "source": [
        "def train_squad_with_layers(num_layers):\n",
        "    model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "    modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "\n",
        "    output_dir = f\"./squad_checkpoints_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"squad\", modified_model, train_squad, eval_squad, output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_conll_with_layers(num_layers):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
        "    modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "\n",
        "    output_dir = f\"./conll_checkpoints_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"conll\", modified_model, train_conll, eval_conll, output_dir)\n"
      ],
      "metadata": {
        "id": "JT-ecVxrgO4W"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store accuracies for each task and layer type\n",
        "results = {\n",
        "    'sst2': {'attention': [], 'ff': [], 'embedding': []},\n",
        "    'squad': {'attention': [], 'ff': [], 'embedding': []},\n",
        "    'conll': {'attention': [], 'ff': [], 'embedding': []}\n",
        "}\n",
        "\n",
        "# Iterate over number of layers (0 to 3 layers)\n",
        "for i in range(4):\n",
        "    # SST-2 Task (Sentiment classification)\n",
        "    results['sst2']['attention'].append(train_sst2_with_layers(num_layers=i))  # Self-Attention layers for SST-2\n",
        "    results['sst2']['ff'].append(train_sst2_with_layers(num_layers=i))  # Feed-Forward layers for SST-2\n",
        "    results['sst2']['embedding'].append(train_sst2_with_layers(num_layers=i))  # Embedding layers for SST-2\n",
        "\n",
        "    # SQuAD Task (Question Answering)\n",
        "    results['squad']['attention'].append(train_squad_with_layers(num_layers=i))  # Self-Attention layers for SQuAD\n",
        "    results['squad']['ff'].append(train_squad_with_layers(num_layers=i))  # Feed-Forward layers for SQuAD\n",
        "    results['squad']['embedding'].append(train_squad_with_layers(num_layers=i))  # Embedding layers for SQuAD\n",
        "\n",
        "    # CoNLL-2003 Task (NER)\n",
        "    results['conll']['attention'].append(train_conll_with_layers(num_layers=i))  # Self-Attention layers for CoNLL\n",
        "    results['conll']['ff'].append(train_conll_with_layers(num_layers=i))  # Feed-Forward layers for CoNLL\n",
        "    results['conll']['embedding'].append(train_conll_with_layers(num_layers=i))  # Embedding layers for CoNLL\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "j834Q227gOzp",
        "outputId": "2362736d-83b6-486e-8337-2f78e0fe55cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='8420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   2/8420 : < :, Epoch 0.00/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = range(4)  # Number of layers (0 to 3)\n",
        "\n",
        "# Create 9 separate plots\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# SST-2 (Sentiment Classification)\n",
        "plt.subplot(3, 3, 1)\n",
        "plt.plot(x, results['sst2']['attention'], marker='o')\n",
        "plt.title('SST-2: Self-Attention Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 2)\n",
        "plt.plot(x, results['sst2']['ff'], marker='o')\n",
        "plt.title('SST-2: Feed-Forward Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 3)\n",
        "plt.plot(x, results['sst2']['embedding'], marker='o')\n",
        "plt.title('SST-2: Embedding Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# SQuAD (Question Answering)\n",
        "plt.subplot(3, 3, 4)\n",
        "plt.plot(x, results['squad']['attention'], marker='o')\n",
        "plt.title('SQuAD: Self-Attention Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 5)\n",
        "plt.plot(x, results['squad']['ff'], marker='o')\n",
        "plt.title('SQuAD: Feed-Forward Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 6)\n",
        "plt.plot(x, results['squad']['embedding'], marker='o')\n",
        "plt.title('SQuAD: Embedding Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# CoNLL-2003 (NER)\n",
        "plt.subplot(3, 3, 7)\n",
        "plt.plot(x, results['conll']['attention'], marker='o')\n",
        "plt.title('CoNLL-2003: Self-Attention Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.plot(x, results['conll']['ff'], marker='o')\n",
        "plt.title('CoNLL-2003: Feed-Forward Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.plot(x, results['conll']['embedding'], marker='o')\n",
        "plt.title('CoNLL-2003: Embedding Layers')\n",
        "plt.xlabel('Number of Layers')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2JCiuW-dgOpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}