{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKWuf4ySsHoN",
        "outputId": "a5668066-931e-4dc2-be49-ca251f772e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch matplotlib evaluate scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "\n",
        "# Load the SST-2, SQuAD, and CoNLL-2003 datasets\n",
        "dataset_sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "dataset_squad = load_dataset(\"squad\")\n",
        "dataset_conll = load_dataset(\"conll2003\")\n",
        "\n",
        "# Load metric (accuracy, precision, recall, f1)\n",
        "metric_accuracy = evaluate.load(\"accuracy\")\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "metric_precision = evaluate.load(\"precision\")\n",
        "metric_recall = evaluate.load(\"recall\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "ROxphKDfsh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_sst2(examples):\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_sst2 = dataset_sst2.map(tokenize_sst2, batched=True)\n",
        "train_sst2 = tokenized_sst2[\"train\"]\n",
        "eval_sst2 = tokenized_sst2[\"validation\"]\n"
      ],
      "metadata": {
        "id": "nFlDjU06sh6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_squad(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"question\"], examples[\"context\"], truncation=True, padding=\"max_length\", max_length=384\n",
        "    )\n",
        "\n",
        "tokenized_squad = dataset_squad.map(tokenize_squad, batched=True)\n",
        "train_squad = tokenized_squad[\"train\"]\n",
        "eval_squad = tokenized_squad[\"validation\"]\n"
      ],
      "metadata": {
        "id": "mZXpSkhwsh4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_conll(examples):\n",
        "    return tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "tokenized_conll = dataset_conll.map(tokenize_conll, batched=True)\n",
        "train_conll = tokenized_conll[\"train\"]\n",
        "eval_conll = tokenized_conll[\"validation\"]\n"
      ],
      "metadata": {
        "id": "-2RFIzzash2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertForSequenceClassification, BertForQuestionAnswering, Trainer, TrainingArguments\n",
        "\n",
        "# Define the modified model class to allow adding different types of layers\n",
        "class ModifiedBertModel(nn.Module):\n",
        "    def __init__(self, original_model, additional_attn_layers=0, additional_ff_layers=0, additional_embed_layers=0):\n",
        "        super(ModifiedBertModel, self).__init__()\n",
        "        self.bert = original_model.bert  # Use the pre-trained BERT model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = original_model.classifier\n",
        "\n",
        "        # Add additional self-attention layers\n",
        "        self.extra_attn_layers = nn.ModuleList(\n",
        "            [nn.TransformerEncoderLayer(d_model=768, nhead=12) for _ in range(additional_attn_layers)]\n",
        "        )\n",
        "\n",
        "        # Add additional feed-forward layers\n",
        "        self.extra_ff_layers = nn.ModuleList(\n",
        "            [nn.Linear(768, 768) for _ in range(additional_ff_layers)]\n",
        "        )\n",
        "\n",
        "        # Add additional embedding layers\n",
        "        self.extra_embed_layers = nn.ModuleList(\n",
        "            [nn.Embedding(30522, 768) for _ in range(additional_embed_layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Pass through additional embedding layers\n",
        "        for layer in self.extra_embed_layers:\n",
        "            sequence_output = layer(input_ids)\n",
        "\n",
        "        # Pass through additional self-attention layers\n",
        "        for layer in self.extra_attn_layers:\n",
        "            sequence_output = layer(sequence_output)\n",
        "\n",
        "        # Pass through additional feed-forward layers\n",
        "        for layer in self.extra_ff_layers:\n",
        "            sequence_output = layer(sequence_output)\n",
        "\n",
        "        pooled_output = sequence_output[:, 0]  # Taking the [CLS] token's representation\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n"
      ],
      "metadata": {
        "id": "rd_CWsRhshz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import os\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "# Function to compute the metrics (accuracy, precision, recall, f1)\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
        "    acc = metric_accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Function to train the model and use checkpointing\n",
        "def train_with_checkpointing(task_name, model, train_dataset, eval_dataset, output_dir, num_epochs=2):\n",
        "    # Check if there is an existing checkpoint\n",
        "    last_checkpoint = None\n",
        "    if os.path.exists(output_dir):\n",
        "        checkpoints = [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(\"checkpoint\")]\n",
        "        if checkpoints:\n",
        "            last_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))  # Get checkpoint with highest step\n",
        "            print(f\"Resuming from checkpoint: {last_checkpoint}\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        eval_strategy=\"steps\",  # Make eval strategy match save strategy\n",
        "        save_strategy=\"steps\",  # Set save strategy to steps\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=num_epochs,\n",
        "        save_steps=500,  # Save the model every 500 steps\n",
        "        save_total_limit=3,  # Keep only the last 3 checkpoints\n",
        "        logging_dir=f\"./logs_{task_name}\",\n",
        "        logging_steps=100,\n",
        "        load_best_model_at_end=True,  # Load the best model after training\n",
        "        resume_from_checkpoint=last_checkpoint if last_checkpoint else None,  # Resume training from the last checkpoint\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,  # Use the compute_metrics function\n",
        "    )\n",
        "\n",
        "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
        "    eval_result = trainer.evaluate()\n",
        "    return eval_result\n"
      ],
      "metadata": {
        "id": "kmP4cpuEshxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sst2_with_layers(num_layers, layer_type):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "    # Decide which type of layer to add\n",
        "    if layer_type == 'attention':\n",
        "        modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "    elif layer_type == 'ff':\n",
        "        modified_model = ModifiedBertModel(model, additional_ff_layers=num_layers)\n",
        "    elif layer_type == 'embedding':\n",
        "        modified_model = ModifiedBertModel(model, additional_embed_layers=num_layers)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
        "\n",
        "    output_dir = f\"./sst2_checkpoints_{layer_type}_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"sst2\", modified_model, train_sst2, eval_sst2, output_dir)\n"
      ],
      "metadata": {
        "id": "trkSMAg7shue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_squad_with_layers(num_layers, layer_type):\n",
        "    model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Decide which type of layer to add\n",
        "    if layer_type == 'attention':\n",
        "        modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "    elif layer_type == 'ff':\n",
        "        modified_model = ModifiedBertModel(model, additional_ff_layers=num_layers)\n",
        "    elif layer_type == 'embedding':\n",
        "        modified_model = ModifiedBertModel(model, additional_embed_layers=num_layers)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
        "\n",
        "    output_dir = f\"./squad_checkpoints_{layer_type}_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"squad\", modified_model, train_squad, eval_squad, output_dir)\n"
      ],
      "metadata": {
        "id": "OfC9QZGishrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_conll_with_layers(num_layers, layer_type):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
        "\n",
        "    # Decide which type of layer to add\n",
        "    if layer_type == 'attention':\n",
        "        modified_model = ModifiedBertModel(model, additional_attn_layers=num_layers)\n",
        "    elif layer_type == 'ff':\n",
        "        modified_model = ModifiedBertModel(model, additional_ff_layers=num_layers)\n",
        "    elif layer_type == 'embedding':\n",
        "        modified_model = ModifiedBertModel(model, additional_embed_layers=num_layers)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown layer type: {layer_type}\")\n",
        "\n",
        "    output_dir = f\"./conll_checkpoints_{layer_type}_{num_layers}_layers\"\n",
        "    return train_with_checkpointing(\"conll\", modified_model, train_conll, eval_conll, output_dir)\n"
      ],
      "metadata": {
        "id": "p-WUzQEjshpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to store accuracies for each task and layer type\n",
        "results = {\n",
        "    'sst2': {'accuracy': [], 'f1': [], 'precision': [], 'recall': []},\n",
        "    'squad': {'accuracy': [], 'f1': [], 'precision': [], 'recall': []},\n",
        "    'conll': {'accuracy': [], 'f1': [], 'precision': [], 'recall': []}\n",
        "}\n",
        "\n",
        "# Iterate over number of layers (0 to 3 layers) and add different types of layers\n",
        "for i in range(4):\n",
        "    # SST-2 Task (Sentiment classification)\n",
        "    eval_result = train_sst2_with_layers(num_layers=i, layer_type='attention')  # Self-Attention layers for SST-2\n",
        "    results['sst2']['accuracy'].append(eval_result['eval_accuracy'])\n",
        "    results['sst2']['f1'].append(eval_result['eval_f1'])\n",
        "    results['sst2']['precision'].append(eval_result['eval_precision'])\n",
        "    results['sst2']['recall'].append(eval_result['eval_recall'])\n",
        "\n",
        "    # SQuAD Task (Question Answering)\n",
        "    eval_result = train_squad_with_layers(num_layers=i, layer_type='attention')  # Self-Attention layers for SQuAD\n",
        "    results['squad']['accuracy'].append(eval_result['eval_accuracy'])\n",
        "    results['squad']['f1'].append(eval_result['eval_f1'])\n",
        "    results['squad']['precision'].append(eval_result['eval_precision'])\n",
        "    results['squad']['recall'].append(eval_result['eval_recall'])\n",
        "\n",
        "    # CoNLL-2003 Task (NER)\n",
        "    eval_result = train_conll_with_layers(num_layers=i, layer_type='attention')  # Self-Attention layers for CoNLL\n",
        "    results['conll']['accuracy'].append(eval_result['eval_accuracy'])\n",
        "    results['conll']['f1'].append(eval_result['eval_f1'])\n",
        "    results['conll']['precision'].append(eval_result['eval_precision'])\n",
        "    results['conll']['recall'].append(eval_result['eval_recall'])\n"
      ],
      "metadata": {
        "id": "0B2ihC4xshmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = range(4)  # Number of layers (0 to 3)\n",
        "\n",
        "metrics = ['accuracy', 'f1', 'precision', 'recall']\n",
        "\n",
        "# SST-2 Task\n",
        "for metric in metrics:\n",
        "    # Self-Attention\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['sst2'][metric], marker='o')\n",
        "    plt.title(f'SST-2: {metric.capitalize()} (Self-Attention)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Feed-Forward\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['sst2'][metric], marker='o')\n",
        "    plt.title(f'SST-2: {metric.capitalize()} (Feed-Forward)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Embedding\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['sst2'][metric], marker='o')\n",
        "    plt.title(f'SST-2: {metric.capitalize()} (Embedding)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "# SQuAD Task\n",
        "for metric in metrics:\n",
        "    # Self-Attention\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['squad'][metric], marker='o')\n",
        "    plt.title(f'SQuAD: {metric.capitalize()} (Self-Attention)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Feed-Forward\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['squad'][metric], marker='o')\n",
        "    plt.title(f'SQuAD: {metric.capitalize()} (Feed-Forward)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Embedding\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['squad'][metric], marker='o')\n",
        "    plt.title(f'SQuAD: {metric.capitalize()} (Embedding)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "# CoNLL-2003 Task\n",
        "for metric in metrics:\n",
        "    # Self-Attention\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['conll'][metric], marker='o')\n",
        "    plt.title(f'CoNLL-2003: {metric.capitalize()} (Self-Attention)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Feed-Forward\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['conll'][metric], marker='o')\n",
        "    plt.title(f'CoNLL-2003: {metric.capitalize()} (Feed-Forward)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n",
        "\n",
        "    # Embedding\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(x, results['conll'][metric], marker='o')\n",
        "    plt.title(f'CoNLL-2003: {metric.capitalize()} (Embedding)')\n",
        "    plt.xlabel('Number of Layers')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "fOxh5Td5shjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}